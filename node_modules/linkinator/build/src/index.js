"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const events_1 = require("events");
const gaxios = require("gaxios");
const http = require("http");
const enableDestroy = require("server-destroy");
const p_queue_1 = require("p-queue");
const links_1 = require("./links");
const url_1 = require("url");
const finalhandler = require("finalhandler");
const serveStatic = require("serve-static");
var LinkState;
(function (LinkState) {
    LinkState["OK"] = "OK";
    LinkState["BROKEN"] = "BROKEN";
    LinkState["SKIPPED"] = "SKIPPED";
})(LinkState = exports.LinkState || (exports.LinkState = {}));
/**
 * Instance class used to perform a crawl job.
 */
class LinkChecker extends events_1.EventEmitter {
    /**
     * Crawl a given url or path, and return a list of visited links along with
     * status codes.
     * @param options Options to use while checking for 404s
     */
    async check(options) {
        options.linksToSkip = options.linksToSkip || [];
        let server;
        if (!options.path.startsWith('http')) {
            const port = options.port || 5000 + Math.round(Math.random() * 1000);
            server = await this.startWebServer(options.path, port);
            enableDestroy(server);
            options.path = `http://localhost:${port}`;
        }
        const queue = new p_queue_1.default({
            concurrency: options.concurrency || 100,
        });
        const results = new Array();
        const url = new url_1.URL(options.path);
        const initCache = new Set();
        initCache.add(url.href);
        queue.add(async () => {
            await this.crawl({
                url: new url_1.URL(options.path),
                crawl: true,
                checkOptions: options,
                results,
                cache: initCache,
                queue,
            });
        });
        await queue.onIdle();
        const result = {
            links: results,
            passed: results.filter(x => x.state === LinkState.BROKEN).length === 0,
        };
        if (server) {
            server.destroy();
        }
        return result;
    }
    /**
     * Spin up a local HTTP server to serve static requests from disk
     * @param root The local path that should be mounted as a static web server
     * @param port The port on which to start the local web server
     * @private
     * @returns Promise that resolves with the instance of the HTTP server
     */
    startWebServer(root, port) {
        return new Promise((resolve, reject) => {
            const serve = serveStatic(root);
            const server = http
                .createServer((req, res) => 
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            serve(req, res, finalhandler(req, res)))
                .listen(port, () => resolve(server))
                .on('error', reject);
        });
    }
    /**
     * Crawl a given url with the provided options.
     * @pram opts List of options used to do the crawl
     * @private
     * @returns A list of crawl results consisting of urls and status codes
     */
    async crawl(opts) {
        // explicitly skip non-http[s] links before making the request
        const proto = opts.url.protocol;
        if (proto !== 'http:' && proto !== 'https:') {
            const r = {
                url: opts.url.href,
                status: 0,
                state: LinkState.SKIPPED,
                parent: opts.parent,
            };
            opts.results.push(r);
            this.emit('link', r);
            return;
        }
        // Check for a user-configured function to filter out links
        if (typeof opts.checkOptions.linksToSkip === 'function' &&
            (await opts.checkOptions.linksToSkip(opts.url.href))) {
            const result = {
                url: opts.url.href,
                state: LinkState.SKIPPED,
                parent: opts.parent,
            };
            opts.results.push(result);
            this.emit('link', result);
            return;
        }
        // Check for a user-configured array of link regular expressions that should be skipped
        if (Array.isArray(opts.checkOptions.linksToSkip)) {
            const skips = opts.checkOptions.linksToSkip
                .map(linkToSkip => {
                return new RegExp(linkToSkip).test(opts.url.href);
            })
                .filter(match => !!match);
            if (skips.length > 0) {
                const result = {
                    url: opts.url.href,
                    state: LinkState.SKIPPED,
                    parent: opts.parent,
                };
                opts.results.push(result);
                this.emit('link', result);
                return;
            }
        }
        // Perform a HEAD or GET request based on the need to crawl
        let status = 0;
        let state = LinkState.BROKEN;
        let data = '';
        let shouldRecurse = false;
        let res = undefined;
        try {
            res = await gaxios.request({
                method: opts.crawl ? 'GET' : 'HEAD',
                url: opts.url.href,
                responseType: opts.crawl ? 'text' : 'stream',
                validateStatus: () => true,
                timeout: opts.checkOptions.timeout,
            });
            // If we got an HTTP 405, the server may not like HEAD. GET instead!
            if (res.status === 405) {
                res = await gaxios.request({
                    method: 'GET',
                    url: opts.url.href,
                    responseType: 'stream',
                    validateStatus: () => true,
                    timeout: opts.checkOptions.timeout,
                });
            }
        }
        catch (err) {
            // request failure: invalid domain name, etc.
            // this also occasionally catches too many redirects, but is still valid (e.g. https://www.ebay.com)
            // for this reason, we also try doing a GET below to see if the link is valid
        }
        try {
            //some sites don't respond to a stream response type correctly, especially with a HEAD. Try a GET with a text response type
            if ((res === undefined || res.status < 200 || res.status >= 300) &&
                !opts.crawl) {
                res = await gaxios.request({
                    method: 'GET',
                    url: opts.url.href,
                    responseType: 'text',
                    validateStatus: () => true,
                    timeout: opts.checkOptions.timeout,
                });
            }
        }
        catch (ex) {
            // catch the next failure
        }
        if (res !== undefined) {
            status = res.status;
            data = res.data;
            shouldRecurse = isHtml(res);
        }
        // Assume any 2xx status is ðŸ‘Œ
        if (status >= 200 && status < 300) {
            state = LinkState.OK;
        }
        const result = {
            url: opts.url.href,
            status,
            state,
            parent: opts.parent,
        };
        opts.results.push(result);
        this.emit('link', result);
        // If we need to go deeper, scan the next level of depth for links and crawl
        if (opts.crawl && shouldRecurse) {
            this.emit('pagestart', opts.url);
            const urlResults = links_1.getLinks(data, opts.url.href);
            for (const result of urlResults) {
                // if there was some sort of problem parsing the link while
                // creating a new URL obj, treat it as a broken link.
                if (!result.url) {
                    const r = {
                        url: result.link,
                        status: 0,
                        state: LinkState.BROKEN,
                        parent: opts.url.href,
                    };
                    opts.results.push(r);
                    this.emit('link', r);
                    continue;
                }
                let crawl = (opts.checkOptions.recurse &&
                    result.url &&
                    result.url.href.startsWith(opts.checkOptions.path));
                // only crawl links that start with the same host
                if (crawl) {
                    try {
                        const pathUrl = new url_1.URL(opts.checkOptions.path);
                        crawl = result.url.host === pathUrl.host;
                    }
                    catch (_a) {
                        // ignore errors
                    }
                }
                // Ensure the url hasn't already been touched, largely to avoid a
                // very large queue length and runaway memory consumption
                if (!opts.cache.has(result.url.href)) {
                    opts.cache.add(result.url.href);
                    opts.queue.add(async () => {
                        await this.crawl({
                            url: result.url,
                            crawl,
                            cache: opts.cache,
                            results: opts.results,
                            checkOptions: opts.checkOptions,
                            queue: opts.queue,
                            parent: opts.url.href,
                        });
                    });
                }
            }
        }
    }
}
exports.LinkChecker = LinkChecker;
/**
 * Convenience method to perform a scan.
 * @param options CheckOptions to be passed on
 */
async function check(options) {
    const checker = new LinkChecker();
    const results = await checker.check(options);
    return results;
}
exports.check = check;
/**
 * Checks to see if a given source is HTML.
 * @param {object} response Page response.
 * @returns {boolean}
 */
function isHtml(response) {
    const contentType = response.headers['content-type'] || '';
    return (!!contentType.match(/text\/html/g) ||
        !!contentType.match(/application\/xhtml\+xml/g));
}
//# sourceMappingURL=index.js.map